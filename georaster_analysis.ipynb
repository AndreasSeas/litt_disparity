{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import scipy.ndimage \n",
    "import shapely.geometry\n",
    "import geocube.api.core \n",
    "import skimage.transform\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import numpy.random as random\n",
    "\n",
    "import xycmap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook will read out data grouped by zip code and create a raster interpolation along with a 2D mask for a particular metropolitan statistical area\n",
    "\n",
    "In the example here, the data is the number of patients evalulated for DBS from each zip code and the population within that zip code whose race is Black (both normalized against total population)\n",
    "The notebook \"download_acs_data.ipynb\" has code to get data from the Census's American Community Survey\n",
    "\n",
    "\n",
    "You will need a python environment with numpy, pandas, geopandas, scipy, shapely, geocube, scikit-image, matplotlib, statsmodels, and xycmap\n",
    "All should be available on conda-forge or pip "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the number of patients by zip code\n",
    "\n",
    "The original data was the number of patients we considered for DBS, broken down by the zip code of their address of record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts = pd.read_csv('dbs_pts_byzip.csv').set_index('zip_code')\n",
    "pts.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next load relevant demographic data, by zip code tabulation area \n",
    "\n",
    "This is public data provided by the Census (American Community Survey). \n",
    "Code to download it is included in \"download_acs_data.ipynb\". There are many types of demographic data available. It is broken down by zip code tabulation area, which is an approximation of the geographic encompassed by each zip code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_total = pd.read_csv('pop_total.csv').set_index('zcta')\n",
    "pop_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_black = pd.read_csv('pop_black.csv').set_index('zcta')\n",
    "pop_black.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the data into a single table and calculate per-capita metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts_pop = pop_total.join(pop_black).join(pts)\n",
    "pts_pop = pts_pop[pts_pop['pop_total']>0]\n",
    "pts_pop['pt_count'] = pts_pop['pt_count'].fillna(0).astype(int)\n",
    "\n",
    "#the variables are normalized differently, for display\n",
    "pts_pop['pct_black'] = (pts_pop['pop_black']/pts_pop['pop_total'])*100\n",
    "pts_pop['pts_per_100k'] = (pts_pop['pt_count']/pts_pop['pop_total'])*100000\n",
    "\n",
    "#reduce the data to the variables of interest\n",
    "pts_pop = pts_pop[['pct_black','pts_per_100k']]\n",
    "\n",
    "pts_pop.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, add in geospatial data for each zip code tabulation area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, load the geometry of each zip code tabulation area\n",
    "zipgeom = (gpd.read_file('tl_2020_us_zcta510/tl_2020_us_zcta510.shp')\n",
    "            .rename(columns={'ZCTA5CE10':'zipcode','geometry':'zcta_polygon'}))[['zipcode','zcta_polygon']]\n",
    "zipgeom['zipcode'] = zipgeom['zipcode'].astype(int)\n",
    "zipgeom['zcta_centroid'] = [g.centroid for g in zipgeom['zcta_polygon']]\n",
    "#Remove zip codes with no population\n",
    "zipgeom = zipgeom.join(pop_total,on='zipcode')\n",
    "zipgeom = zipgeom[zipgeom['pop_total']>0].drop(columns=['pop_total'])\n",
    "zipgeom = zipgeom.set_geometry('zcta_polygon')\n",
    "zipgeom.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attach the data of interest to the geospatial geometry\n",
    "pts_pop_geom = zipgeom.join(pts_pop,on='zipcode')\n",
    "pts_pop_geom.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop the geography to a region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, define the lat/lon of the center of our region of interest\n",
    "roi_center_lon, roi_center_lat = -84.32179723327368,33.79198182080914\n",
    "\n",
    "#These approximately convert lat/lon to km\n",
    "km_per_lat = 110.574\n",
    "km_per_lon = (111.320*np.abs(np.cos(roi_center_lat)))\n",
    "\n",
    "#This determines the size of the roi to clip\n",
    "#Should be a square centered at the roi_center extending \"extent_km\" in every direction\n",
    "extent_km = 200\n",
    "extent_lat = extent_km/km_per_lat\n",
    "extent_lon = extent_km/km_per_lon\n",
    "\n",
    "\n",
    "boundspoly = shapely.geometry.Polygon([ (roi_center_lon-extent_lon,roi_center_lat-extent_lat), \n",
    "                                        (roi_center_lon-extent_lon,roi_center_lat+extent_lat), \n",
    "                                        (roi_center_lon+extent_lon,roi_center_lat+extent_lat),\n",
    "                                        (roi_center_lon+extent_lon,roi_center_lat-extent_lat)\n",
    "                                      ])\n",
    "\n",
    "pts_pop_geom_cropped = pts_pop_geom.clip(boundspoly)\n",
    "pts_pop_geom_cropped.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate the data into a raster, smooth with gaussian filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Res/sigma will determine the resolution and degree of smoothing of the plot\n",
    "#A high quality plot will be around res=.1,sigma=100 - this resolution is pretty computationally intensive\n",
    "#can be reduced for a preview\n",
    "res = .1\n",
    "sigma = 10/res\n",
    "\n",
    "\n",
    "#This creates the rasters for each variable\n",
    "datatable = (pts_pop_geom_cropped\n",
    "             .drop(columns=['zipcode'])\n",
    "             .rename(columns={'zcta_polygon':'geometry'})\n",
    "             .set_geometry('geometry'))\n",
    "datacube = geocube.api.core.make_geocube(\n",
    "                    vector_data=datatable,\n",
    "                    resolution=(-res/km_per_lat, res/km_per_lon),\n",
    "                    interpolate_na_method='cubic').fillna(0)\n",
    "\n",
    "#This smooths the rasters for each variable\n",
    "var_names = list(datacube.data_vars)\n",
    "vars_interp = [datacube[vn].to_numpy() for vn in var_names]\n",
    "vars_smooth = [scipy.ndimage.gaussian_filter(v,sigma) for v in vars_interp]\n",
    "\n",
    "\n",
    "#Smoothed rasters are displayed below\n",
    "fig,axes = plt.subplots(1,len(var_names))\n",
    "for name,vs,ax in zip(var_names,vars_smooth,axes):\n",
    "    ax.imshow(vs,cmap='Greens')\n",
    "    ax.set_title(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The center of the roi (defined earlier) is calculated in raster space\n",
    "roi_center_x = np.argmin(np.abs(datacube['x'].to_numpy()-roi_center_lon))\n",
    "roi_center_y = np.argmin(np.abs(datacube['y'].to_numpy()-roi_center_lat))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a mask for the Metropolitan statistical area of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will use the Census code for the MSA/CBSA of interest\n",
    "#12060 is metro Atlanta\n",
    "msa_of_interest = 12060\n",
    "\n",
    "#This is a file that contains a mapping between zip codes and MSA/CBSAs\n",
    "zip_to_msa=pd.read_csv('ZIP_CBSA_122019.csv').set_index('ZIP')[['CBSA']]\n",
    "zip_to_msa.index = zip_to_msa.index.astype(int)\n",
    "\n",
    "\n",
    "#Here, we create a raster with a boolean value for whether each coordinate is in the MSA\n",
    "#This is very similar to the raster of the population/patient variables, with some changes\n",
    "#to accomodate a boolean\n",
    "zips_in_msaoi = zip_to_msa[zip_to_msa['CBSA']==msa_of_interest].index.tolist()\n",
    "\n",
    "zipgeom_msaoi = zipgeom.copy()\n",
    "zipgeom_msaoi['in_msaoi'] = zipgeom['zipcode'].isin(zips_in_msaoi).astype(int)\n",
    "\n",
    "zipgeom_msaoi_cropped = zipgeom_msaoi.clip(boundspoly)\n",
    "\n",
    "msaoi_table = (zipgeom_msaoi_cropped\n",
    "             .drop(columns=['zipcode'])\n",
    "             .rename(columns={'zcta_polygon':'geometry'})\n",
    "             .set_geometry('geometry'))\n",
    "msaoi_cube = geocube.api.core.make_geocube(\n",
    "                    vector_data=msaoi_table,\n",
    "                    resolution=(-res/km_per_lat, res/km_per_lon),\n",
    "                    interpolate_na_method='nearest')\n",
    "msaoi_mask = msaoi_cube['in_msaoi'].to_numpy()\n",
    "\n",
    "\n",
    "plt.imshow(msaoi_mask,cmap='gray')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Statistical analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the rasters into geographic samples for statistics\n",
    "This downscales the raster data such that there are an equal number of pixel samples as\n",
    "there were zip codes, in the MSA of interest\n",
    "This is used to avoid inflating the degrees of freedom for statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smp_cnt_orig = np.sum(zipgeom_msaoi['in_msaoi'])\n",
    "smp_cnt_working = np.sum(msaoi_mask)\n",
    "ds = np.ceil(np.sqrt(np.sum(msaoi_mask)/smp_cnt_orig)).astype(int)\n",
    "ds = 0 \n",
    "#We have to optimize for the right downscaling factor, ds\n",
    "while smp_cnt_working>smp_cnt_orig:\n",
    "    ds = ds + 1\n",
    "    msaoi_mask_ds = skimage.transform.downscale_local_mean(msaoi_mask,(ds,ds))>0\n",
    "    smp_cnt_working = np.sum(msaoi_mask_ds)\n",
    "\n",
    "vars_ds = [skimage.transform.downscale_local_mean(vs,(ds,ds)) for vs in vars_smooth]\n",
    "\n",
    "#The sample count of our rasterified,smoothed, and downsampled data should be less than or \n",
    "#equal to the original number of geographic samples\n",
    "ds, smp_cnt_working, smp_cnt_orig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the regression, comparing two variables within these geographic samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign independent and dependent variables, and run a linear regression\n",
    "y = vars_ds[1][msaoi_mask_ds].flatten()\n",
    "X = vars_ds[0][msaoi_mask_ds].flatten()\n",
    "\n",
    "linreg = sm.OLS(y, sm.add_constant(X)).fit()\n",
    "\n",
    "#Plot scatterplot and display linear regression results\n",
    "fig,ax = plt.subplots()\n",
    "ax.scatter(X,y)\n",
    "ax.set_xlim((0,100))\n",
    "ax.set_ylim((0,10))\n",
    "\n",
    "print(linreg.summary(xname=['intercept',var_names[0]],yname=var_names[1]))\n",
    "\n",
    "linreg.pvalues\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a regression line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_b, reg_m = linreg.params\n",
    "reg_p = linreg.pvalues[1]\n",
    "fit_x = np.array([0,100])\n",
    "fit_y = fit_x*reg_m+reg_b\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.scatter(X,y)\n",
    "ax.set_xlim((0,100))\n",
    "ax.set_ylim((0,10))\n",
    "ax.plot(fit_x,fit_y,color='red',linestyle='dashed')\n",
    "ax.text(80,1.4,f'p={reg_p:1.4f}',color='red')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a colormap using xycmap\n",
    "this is fairly computationally intensive at high resolution (~10min to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we pull out our variables (shown above) into x and y\n",
    "sx = vars_smooth[0].flatten()\n",
    "sy = vars_smooth[1].flatten()\n",
    "smask = msaoi_mask.flatten()\n",
    "\n",
    "#Here we define the axis limits for each variable\n",
    "#For instance, here x is a percentage\n",
    "axlims = {'xlims':(0,100)}\n",
    "\n",
    "#This defines aspects of the colormap\n",
    "#Uses the xycmap package\n",
    "corner_colors = (\"lightgrey\", \"blue\", \"red\", \"purple\")\n",
    "n = (7, 7) \n",
    "\n",
    "#This creates the colormap, applies it to both variables, and reshapes it into an image\n",
    "cmap = xycmap.custom_xycmap(corner_colors=corner_colors, n=n)\n",
    "clist = xycmap.bivariate_color(sx=sx, sy=sy, cmap=cmap,**axlims)\n",
    "cimg = np.array(clist.to_list()).reshape(*vars_smooth[0].shape,4)\n",
    "\n",
    "fig,axes = plt.subplots(1,2)\n",
    "axes[0].imshow(cimg)\n",
    "xycmap.bivariate_legend(ax=axes[1], sx=sx, sy=sy, cmap=cmap,**axlims)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an MSA mask with smooth borders and apply it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "border_gsigma = 6\n",
    "border_dilations = 90\n",
    "msaoi_mask_smooth = scipy.ndimage.gaussian_filter(msaoi_mask.astype(float),border_gsigma)\n",
    "msaoi_mask_smooth = scipy.ndimage.binary_dilation(msaoi_mask,iterations=border_dilations).astype(float)\n",
    "plt.imshow(msaoi_mask_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimg_msaoi = cimg.copy()\n",
    "cimg_msaoi[msaoi_mask_smooth==0] = (1,1,1,0)\n",
    "plt.imshow(cimg_msaoi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop the image to the foreground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_indices = np.nonzero(msaoi_mask_smooth)\n",
    "#This finds the first/last pixel that is not maxed out, in both dimensions\n",
    "xmin,xmax = np.flatnonzero(msaoi_mask_smooth.any(0))[[0,-1]]\n",
    "ymin,ymax = np.flatnonzero(msaoi_mask_smooth.any(1))[[0,-1]]\n",
    "\n",
    "cimg_msaoi_cropped = cimg_msaoi.copy()\n",
    "cimg_msaoi_cropped = cimg_msaoi_cropped[ymin:ymax,xmin:xmax]\n",
    "\n",
    "plt.imshow(cimg_msaoi_cropped)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the final plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,geoax = plt.subplots()\n",
    "plotax = fig.add_axes([0.28, -.3, 0.26, 0.3])\n",
    "cax = fig.add_axes([0.600, -.20, 0.25, 0.25])\n",
    "\n",
    "dred = np.array((119,1,17,255))/255\n",
    "\n",
    "#Plot the geospatial imgage\n",
    "geoax.imshow(cimg_msaoi_cropped)\n",
    "geoax.plot(roi_center_x-xmin,roi_center_y-ymin,\n",
    "                color='gold',markeredgecolor='black',marker='*', \n",
    "                markersize=25,zorder=101)\n",
    "\n",
    "geoax.axis('off')\n",
    "\n",
    "#Plot the colormap legend\n",
    "cax = xycmap.bivariate_legend(ax=cax, sx=sx, sy=sy, cmap=cmap,xlims=(0,100))\n",
    "cax.spines['right'].set_visible(False)\n",
    "cax.spines['top'].set_visible(False)\n",
    "cax.spines['left'].set_visible(False)\n",
    "cax.spines['bottom'].set_visible(False)\n",
    "cax.set_xticks(cax.get_xlim())\n",
    "cax.set_xticklabels([0,100])\n",
    "cax.set_yticks(cax.get_ylim())\n",
    "cax.set_yticklabels([0,15])\n",
    "cax.set_ylabel('DBS Patients/100k',fontsize=10)\n",
    "cax.set_xlabel('% Population Black',fontsize=10)\n",
    "\n",
    "\n",
    "#Plot the scatterplot of geographic samples used for stats\n",
    "plotax.scatter(X,y,color='black',s=1.5)\n",
    "plotax.set_xlim((0,100))\n",
    "plotax.plot(fit_x,fit_y,color=dred,marker=None,linestyle='solid')\n",
    "plotax.text(50,4,f'p={reg_p:1.4f}',color=dred)\n",
    "\n",
    "plotax.set_xlabel('% Population Black',fontsize=10)\n",
    "plotax.set_ylabel('DBS Patients/100K',fontsize=10)\n",
    "\n",
    "plotax.spines['right'].set_visible(False)\n",
    "plotax.spines['top'].set_visible(False)\n",
    "\n",
    "\n",
    "fig.subplots_adjust(wspace=0, hspace=0,bottom=0,top=1,left=0, right=1) \n",
    "fig.tight_layout()\n",
    "\n",
    "#This labels each subplot\n",
    "laxes = [geoax,plotax]\n",
    "labels = ['A','B'] \n",
    "offsets = [(0,200),(-.4,11)]\n",
    "for label,offset,ax in zip(labels,offsets,laxes):\n",
    "    ax.text(offset[0],\n",
    "            offset[1],\n",
    "            label,\n",
    "            fontweight='bold',\n",
    "            color='black',backgroundcolor='white')\n",
    "    \n",
    "fig.savefig('georaster_plot.png',dpi=1000,transparent=False,bbox_inches='tight',facecolor='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
